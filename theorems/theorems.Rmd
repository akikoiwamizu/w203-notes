---
title: "Theorems & Definitions"
author: "Akiko Iwamizu"
date: "9/15/2021"
output:
  bookdown::html_document2: default
  bookdown::pdf_document2:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# Probability

::: {.definition name="Event Space"}
A set $S$ of subsets of $\Omega$ is an _event space_ if it satisfies the following:

* *Nonempty*: $S$ $\ne$ $\emptyset$.
* *Closed under complements*: if $A$ $\in$ $S$, then $A^c$ $\in$ $S$.
* *Closed under countable unions*: if $A_1$, $A_2$, $A_3$,... $\in$ $S$, then $A_1$ $\cup$ $A_2$ $\cup$ $A_3$ $\cup$ ... $\in$ $S$.

&nbsp;

:::

::: {.definition name="Kolmogorov Axioms"}
Let $\Omega$ be a _sample space_, $S$ be an _event space_, and $P$ be a _probability measure_. Then ($\Omega$,$S$,$P$) is a _probability space_ if it satifies the following:

* *Non-negativity*: $\forall$$A$ $\in$ $S$, $P(A) \ge 0$ where $P(A)$ is finite and real.
* *Unitary*: $P(\Omega) = 1$.
* *Countable additivity*: if $A_1$, $A_2$, $A_3$,... $\in$ $S$ are pairwise disjoint, then
$$P(A_1 \cup A_2 \cup A_3 \cup ...) = P(A_1) + P(A_2) + P(A_3) +... = \sum P(A_i).$$

The intuition behind these axioms is as follow: The first axiom states that the probability of any event is a non-negative number; there cannot be a less-than-zero chance of an event occuring. The second axiom states that the probability measure of the entire sample space is one. In other words, it is certain that some outcome will occur. Finally, the third axiom states that, given any number of _mutually exclusive events_, the probability that one of those events will occur is the sum of their individual probabilities.

&nbsp;

:::

::: {.definition name="Pairwise Disjoint"}
Recall that sets $A$ and $B$ are disjoint if $A \cap B = \emptyset$. We say that $A_1$, $A_2$, $A_3$,... are pairwise disjoint if each of them is disjoint from every other, that is, $\forall i \ne j, A_i \cap A_j = \emptyset$.

&nbsp;

:::

::: {.theorem name="Basic Properties of Probability"}
Let ($\Omega$,$S$,$P$) be a probability space. Then

* *Monotonicity*: $\forall A, B \in S$, if $A \subseteq B$, then $P(A) \le P(B)$.
  * Monotonicity implies that, if one event is a subset of another (so that the former always occurs whenever the latter does), then the probability of the former occurring is no greather than that of the latter. 
* *Subtraction rule*: $\forall A, B \in S$, if $A \subseteq B$, then $P(B \setminus A) = P(B) - P(A)$.
  * The subtraction rule implies that the probability that the second event occurs but not the first is equal to the probability of the second event minus the probability of the first event. 
* *Zero probability of the empty set*: $P(\emptyset) = 0$.
  * Zero probability of the empty set means that some event in our event space must occur, and probability bounds mean that each of these events has some probability of occuring between zero and one. 
* *Probability bounds*: $\forall A \in S$, $0 \le P(A) \le 1$.
  * Monotonicity and unitarity (and non-negativity) imply the probability bounds since $A \subseteq \Omega$.
* *Complement rule*: $\forall A \in S$, $P(A^c) = 1 - P(A)$.
  * The complement rule implies that the probability of any of these events not occurring is one minus the probability of the event occurring - so that the probability that a given event either occurs or does not occur is one.

&nbsp;

:::

::: {.definition name="Joint Probability"}
For $A, B \in S$, the _joint probability_ of $A$ and $B$ is $P(A \cap B)$.

In other words, the joint probability of two events $A$ and $B$ is the probability of the intersection of $A$ and $B$ (which is itself an event in $S$), that is, the set of all states of the world in which both $A$ and $B$ occur.

&nbsp;

:::

::: {.theorem name="Addition Rule"}
For $A, B \in S$, 
$$P(A \cup B) = P(A) + P(B) - P(A \cap B).$$

&nbsp;

:::

::: {.definition name="Conditional Probability"}
For $A, B \in S$ with $P(B) > 0$, the _conditional probability_ of $A$ given $B$ is
$$P(A \mid B) = \frac {P(A \cap B)}{P(B)}.$$

&nbsp;

:::

::: {.theorem name="Multiplicative Law of Probability"}
For $A, B \in S$ with $P(B) > 0$,
$$P(A \mid B)P(B) = P(A \cap B).$$
$$P(A \cap B) = P(B|A)P(A).$$

&nbsp;

:::

::: {.theorem name="Bayes' Rule"}
For $A, B \in S$ with $P(A) > 0$ and $P(B) > 0$,
$$P(A \mid B) = \frac {P(B \mid A)P(A)}{P(B)}.$$

&nbsp;

:::

::: {.definition name="Partition Rule 1"}
If $A_1$, $A_2$, $A_3$,... $\in$ $S$ are nonempty and pairwise disjoint, and $\Omega = A_1 \cup A_2 \cup A_3 \cup ...$, then {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$.

$$A = A \cap (B \cup B^c) = (A \cap B) \cup (A \cap B^c) = (A \cap B) \cup (A \setminus B).$$

&nbsp;

:::

::: {.definition name="Partition Rule 2"}
If $A_1$, $A_2$, $A_3$,... $\in$ $S$ are nonempty and pairwise disjoint, and $\Omega = A_1 \cup A_2 \cup A_3 \cup ...$, then {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$.
$$A \cup B = [(A \setminus B) \cup (A \cap B)] \cup [(A \cap B) \cup (B \setminus A)] = (A \setminus B) \cup (A \cap B) \cup (B \setminus A).$$

&nbsp;

:::

::: {.theorem name="Law of Total Probability"}
If {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$ and $B \in S$, then
$$P(B) = \sum P(B \cap A_i).$$

If we also have $P(A_i) > 0$ for $i$ = 1,2,3,..., then this can also be stated as
$$P(B) = \sum P(B \mid A_i)P(A_i).$$

&nbsp;

:::

::: {.theorem name="Alternative Forms of Bayes' Rule"}
If {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$ with $P(A_i) > 0$ for $i$ = 1,2,3,..., and $B \in S$, then apply the Law of Total Probability to the denominator to get

$$P(A_j \mid B) = \frac {P(B \mid A_j)P(A_j)}{\sum P(B \cap A_i)}.$$
$$P(A_j \mid B) = \frac {P(B \mid A_j)P(A_j)}{\sum P(B \mid A_i)P(A_i)}.$$
$$P(A \mid B) = \frac {P(B \mid A)P(A)}{P(B \mid A)P(A) + P(B \mid A^c)P(A^c)}.$$

&nbsp;

:::

::: {.definition name="Independence of Events"}
Events $A,B \in S$ are _independent_ if 
$$P(A \cap B) = P(A)P(B).$$

&nbsp;

:::

::: {.theorem name="Conditional Probability and Independence"}
For $A,B \in S$ with $P(B) > 0$, $A$ and $B$ are independent if and only if 
$$P(A \mid B) = P(A).$$

&nbsp;

:::


# Random Variables

::: {.definition name="Random Variable"}
A _random variable_ is a function $X: \Omega \to \mathbb{R}$ such that 

$$\forall r \in \mathbb{R}, \{\omega \in \Omega: X(\omega) \leq r \} \in S.$$ 

Where each $\omega \in \Omega$ denotes a state of the world, which may be represented by anything: numbers, letters, words, etc. to describe all the distinct possible outcomes that could occur. A _random variable_ maps each of these states of the world to a real number. Thus, it is often remarked that, a _random variable_ is neither random nor a variable, as it is merely a _function_. So when the state of the world is $\omega$, the random variable takes on the value $X(\omega)$. For example, the event $\{X = 1\}$ should be understood to mean the set of states $\{\omega \in \Omega: X(\omega) = 1\}$.
:::

::: {.definition name="Function of a Random Variable"}
Let $g: U \to \mathbb{R}$ be a function, where $X(\Omega) \subseteq U \subseteq \mathbb{R}$. Then, if $g \circ X: \Omega \to \mathbb{R}$ is a random variable, we say that $g$ is a _function of X_, and write $g(X)$ to denote the randome variable $g \circ X$. This general definition allows us to formally work with transformations of random variables as random variables in their own right. 
:::

::: {.definition name="Operator on a Random Variable"}
An _operator_ $A$ on a random variable maps the function $X(\cdot)$ to a real number, denoted by $A[X]$.
:::

::: {.example name="Example of a Random Variable"}
We can define events in $S$ in terms of a random variable $X$. For example we could let

* $A = \{\omega \in \Omega: X(\omega) = 1\} = \{X = 1\}.$
* $B = \{\omega \in \Omega: X(\omega) \geq 0\} = \{X \geq 0\}.$
* $C = \{\omega \in \Omega: X(\omega)^2 \le 10, X(\omega) \neq 3\} = \{X(\omega)^2 \le 10, X(\omega) \neq 3\}.$
* $PR[X = 1] = P(A).$
* $PR[X \geq 0] = P(B).$
* $PR[X(\omega)^2 \le 10, X(\omega) \neq 3] = P(C).$
:::

::: {.definition name="Discrete Random Variable"}
A random variable $X$ is _discrete_ if its range, $X(\Omega)$, is a countable set. In other words, a _discrete random variable_ is a random variable that can only take on a finite or countably infinite number of different values.
:::

::: {.definition name="Probability Mass Function (PMF)"}
For a discrete random variable $X$, the _probability mass function_ of $X$ is:

$$f(x) = Pr[X = x], \forall x \in \mathbb{R}.$$
:::

::: {.example name="A Fair Die Roll"}
We can apply the definition of _PMF_ to our familiar die roll example. Consider a roll of one fair (six-sided) die. Let $X$ take on the value of the outcome of the die roll; that is, let $\Omega = \{1,2,3,4,5,6\}$ and $X(\omega) = \omega, \forall \omega \in \Omega$. Then $Pr[X = 1] = Pr[X = 2] = ... = Pr[X = 6] = \frac{1}{6}$. Out of many die rolls, we expect each of the values 1 through 6 to come up one sixth of the time. Thus, the _PMF_ of $X$ is:

$$f(x) = \begin{cases} \frac{1}{6} & : x \in \{1,2,3,4,5,6\} \\ 0 & : otherwise. \end{cases}$$
:::

::: {.example name="A Biased Coin Flip (Bernoulli Distribution)"}
We can also highlight a generalization of our coin flip example: a biased coin flip where the resuling random variable is known as a _Bernoulli_ random variable. Consider a coin flip with a (potentially) biased coin - that is, a coin that comes up tails with some unknown probability. Let $X = 0$ if the coin comes up heads and $X = 1$ if the coin comes up tails; that is, let $\Omega = \{H,T\}$ and let $X(H) = 0$ and $X(T) = 1$. Let $p$ be the probability that the coin comes up tails: $Pr[X = 1] = p$ and let $Pr[X = 0] = 1 - Pr[X = 1] = 1 - p$. Out of many coin flips, we expect that the proportion of times the coin comes up tails will be $p$ and the proportion of the times the coin comes up heads will be $1 - p$. The random variable $X$ thus has a _PMF_ of:

$$f(x) = \begin{cases} 1 - p & : x = 0 \\ p & : x = 1 \\ 0 & : otherwise. \end{cases}$$
:::

::: {.example name="Flipping a Biased Coin Until It Comes Up Tails (Geometric Distribution)"}
This example illustrates how a discrete random variable can have a countably infinite number of possible values. Suppose we flipped a (potentially) biased coin repeatedly until the first time it came up tails. Let $p$ be the probability that the coin comes up tails, and assume $0 < p < 1$. Let $X$ be the number of flips it takes to get tails. For any given positive integer $x$, getting the first tails on exactly the $x^{th}$ flip requires getting heads on each of the first $x - 1$ flips and then tails on the $x^{th}$ flip. The probability of this happening is $(1 - p)^{x - 1}p$, or the product of the probabilities of the desired outcome on each flip (since the flips are independent). So the _PMF_ of $X$ is:

$$f(x) = \begin{cases} (1 - p)^{x - 1}p & : x \in \mathbb{N} \\ 0 & : otherwise. \end{cases}$$
For example, if it is a fair coin (that is, $p = 1 - p = \frac{1}{2}$) then $\forall x \in \mathbb{N}, Pr[X = x] = (\frac{1}{2})^{x}$; the first tails will be obtained on the first flip with probability $\frac{1}{2}$, on the second flip with probability $\frac{1}{4}$, on the third flip with probability $\frac{1}{8}$, and so on. Thus, $X$ can take on _any_ positive integer value, albeit with vanishingly small probability for large values.
:::

::: {.example name="PMF & the Distribution of Discrete Random Variables"}
For a discrete random variable $X$, the _PMF_ tells us everything about its _distribution_, which is loosely defined as the collection of probabilities assigned to events that can be defined just in terms of $X$. To illustrate how the _PMF_ can fully describe a discrete random variable, consider a random variable $X$ such that $f(x) = 0,  \forall x \notin \mathbb{Z}$ (that is, $X$ takes on only integer values). Then:

* $Pr[X \geq 3] = \sum_{x=3}^{\infty}f(x).$
* $Pr[X \geq 3 or X = 1] = f(1) + \sum_{x=3}^{\infty}f(x).$
* $Pr[X < 4] = \sum_{x=1}^{3}f(x).$

For example, for a fair (six-sided) die roll:

* $Pr[X \geq 3] = \sum_{x=3}^{6}f(x) = \frac{4}{6} = \frac{2}{3}.$
* $Pr[X \geq 3 or X = 1] = f(1) + \sum_{x=3}^{6}f(x) = \frac{1}{6} + \frac{4}{6} = \frac{5}{6}.$
* $Pr[X < 4] = \sum_{x=1}^{3}f(x) = \frac{3}{6} = \frac{1}{2}.$
:::


::: {.theorem name="Properties of PMFs"}
For a discrete random variable $X$ with _PMF_ $f$:

* $\forall x \in \mathbb{R}, f(x) \geq 0.$
* $\sum_{x \in X(\Omega)}f(x) = 1.$

The proof of this theorem follows directly from the _Kolmogorov axioms_.  
:::


::: {.theorem name="Event Probabilities for Discrete Random Variables"}
More generally, this theorem gives the formula for using the _PMF_ to compute the probability of _any_ event defined in terms of a discrete random variable $X$. For a discrete random variable $X$ with _PMF_ $f$, if $D \subseteq \mathbb{R}$ and $A = \{X \in D\}$, then:

$$P(A) = Pr[X \in D] = \sum_{x \in X(A)}f(x).$$

The proof of this theorem follows directly from the _Kolmogorov axioms_. Note that any condiiton on $X$ can be expressed as $X \in D$ for some set $D \subseteq \mathbb{R}$, so this theorem allows us to compute the probability of any event defined in terms of a discrete random variable $X$.
:::

::: {.definition name="Cumulative Distribution Function (CDF)"}
For a random variable $X$, the _cumulative distribution function_ of $X$ is:

$$F(x) = Pr[X \leq x], \forall x \in \mathbb{R}.$$
The CDF returns the probability that an outcome for a random variable will be less than or equal to a given value. Importantly, given any random variable, the CDF tells us everything there is to know about its behavior. For any event that can be described in terms of a random variable, we can derive the event's probability from the CDF of the random variable alone.
:::


::: {.theorem name="Properties of CDFs"}
The following important properties of CDFs follow immediately from the axioms and basic properties of probability. For a random variable $X$ with CDF $F$,

* $F$ is nondecreasing: $\forall x_1, x_2 \in \mathbb{R}$, if $x_1 < x_2$, then $F(x_1) \leq F(x_2)$.
* $\lim_{x \to -\infty} F(x) = 0$.
* $\lim_{x \to \infty} F(x) = 1$.
* $\forall x \in \mathbb{R}, 1-F(x) = Pr[X > x]$.
:::


::: {.example name="CDF of a Fair Die Roll"}
For discrete random variables such as a die roll, the CDF is necessarily a step function, that is, a function that is flat everywhere except where it "jumps" from one value to another as the CDF increases by \frac{1}{6} at each value in {1,2,3,4,5,6}. We can evaluate the CDF at any real value, for example,

* $F(-1) = 0$.
* $F(1) = \frac{1}{6}$.
* $F(1.5) = \frac{1}{6}$.
* $F(2) = \frac{2}{6} = \frac{1}{3}$.
* $F(6) = 1$.
* $F(7) = 1$.
* $F(24,603) = 1$.

Note that, in this example, the value of the CDF for any $x$ greater than or equal to 6 will be 1; since any outcome we get is guaranteed to be less than or equal to 6, or 7, or 24,603. Additionally, we can use the CDF to compute probabilities:

* $Pr[X < 2] = Pr[X \leq 1] = F(1) = \frac{1}{6}$,
* $Pr[X \geq 3] = 1- Pr[X < 3] = 1- Pr[X \leq 2] = 1 - F(2) = 1 - \frac{2}{6} = \frac{4}{6} = \frac{2}{3}$,
* $Pr[2 \leq X \leq 4] = Pr[X \leq 4] - Pr[X < 2] = F(4) - F(1) = \frac{4}{6} - \frac{1}{6} = \frac{3}{6} = \frac{1}{2}$,

where the second line applies the _Complement Rule_ and the third applies the _Subtraction Rule_.
:::


::: {.definition name="Continuous Random Variable"}
A random variable $X$ is _continuous_ if there exists a non-negative function $f: \mathbb{R} \to \mathbb{R}$ such that the CDF of $X$ is:

$$F(X) = Pr[X \leq x] = \int_{-\infty}^x f(u)du, \forall x \in \mathbb{R}.$$
:::


::: {.definition name="Probability Density Function (PDF)"}
The function $f$ is called the _probability density function (PDF)_. The Fundamental Theorem of Calculus implies that, for any continuous random variable, the PDF is unique and defined as below. For a continuous random variable $X$ with CDF $F$, the _probability density function_ of $X$ is:

$$f(x) = \left.\frac{dF(u)}{du} \right\vert_{u=x}, \forall x \in \mathbb{R}.$$
Conceptually, the PDF is the continuous analog to the PMF in that it describes how the CDF changes with $x$. The difference is that, whereas PMF specifies the size of the "jump" in the CDF at a point $x$, a PDF gives the instantaneous slope (derivative) of the CDF at a point $x$. That is, for a very small number of $\epsilon > 0$, if we moved from $x$ to $x + \epsilon$, the CDF would change by appox $\epsilon f(x)$.
:::


::: {.theorem name="Properties of PDFs"}
For a continuous random variable $X$ with PDF $f$,

* $\forall x \in \mathbb{R}, f(x) \geq 0$.
* $\int_{-\infty}^{\infty} f(x)dx = 1$.
:::


::: {.theorem name="Properties of PDFs"}
This theorem establishes that we can express any event of the form $\{X \in I\}$, where $I$ is an interval in \mathbb{R}, in terms of integrals of the PDF. For a continuous random variable $X$ with PDF $f$,

* $\forall x \in \mathbb{R}, Pr[X = x] = 0$.
* $\forall x \in \mathbb{R}, Pr[X < x] = Pr[X \leq x] = F(x) = \int_{-\infty}^{x} f(u)du$.
* $\forall x \in \mathbb{R}, Pr[X > x] = Pr[X \geq x] = 1 - F(x) = \int_{x}^{\infty} f(u)du$.
* $\forall a,b \in \mathbb{R}$ with $a \leq b$,

$$\begin{align} Pr[a < X < b] &= Pr[a \leq X < b] \\ &= Pr[a < X \leq b] \\ &= Pr[a \leq X \leq b] \\ &= F(b) - F(a) \\ &= \int_a^b f(x)dx. \end{align}$$
:::


::: {.example name="Standard Uniform Distribution"}
Consider the standard uniform distribution, denoted by $U(0,1)$. Informally, if a random variable $X$ follows the standard uniform distribution, $X$ takes on a random real value from the internal [0,1], with all values in this interval equally likely to occur. We write $X \thicksim U(0,1)$ to denote that $X$ follows the standard uniform distribution. 

The PDF of $U(0,1)$ is:

$$f(x) = \begin{cases} 1 & : 0 \leq x \leq 1 \\ 0 & : otherwise.  \end{cases}$$

The CDF of $U(0,1) is$:

$$F(x) = Pr[X \leq x] = \int_{-\infty}^x f(u)du = \begin{cases} 0 & : x < 0 \\ x & : 0 \leq x \leq 1 \\ 1 & : x > 1.  \end{cases}$$
:::


::: {.example name="Standard Normal Distribution"}
You may be familiar with the standard normal (or Gaussian) distribution. The standard normal distribution is denoted by $N(0,1)$.

The PDF of $N(0,1)$ is:

$$\phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.$$

The CDF of $N(0,1) is$:

$$\Phi(x) = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-\frac{u^2}{2}}du.$$
:::


::: {.definition name="Support"}
For a random variable $X$ with PMF/PDF $f$, the _support_ of $X$ is:

$$Supp[X] = \{x \in \mathbb{R} : f(x) > 0\}.$$

* When the random variable $X$ is _discrete_, $Supp[X]$ is the set of values that $X$ takes on with nonzero probability. For example, if $X$ is the outcome of a die roll, then $Supp[X] = \{1,2,3,4,5,6\}$. 
* When the random variable $X$ is _continuous_, $Supp[X]$ is the set of values over which $X$ has a nonzero probability density. For example, if $X \thicksim U(0,1)$, then $Supp[X] = [0,1] = \{x \in \mathbb{R} : 0 \leq x \leq 1\}$. 
:::


::: {.definition name="Equality of Random Variables"}
When we say two random variables are equal, we mean that they are equal as _functions_; they assign the same value to every state of the world. Let $X$ and $Y$ be random variables. Then $X = Y$ if, $\forall \omega \in \Omega, X(\omega) = Y(\omega)$.
:::


::: {.definition name="Equality of Functions of a Random Variable"}
From the definition above, it follows that two functions of a random variable, $g(X)$ and $h(X)$ are equal as random variables if and only if they are equal as functions on $X(\Omega)$. Let $X$ be a random variable and let $f$ and $g$ be functions of $X$. Then

$$g(X) = h(X) \iff \forall x \in X(\Omega), g(x) = h(x).$$

_Proof_:

* Suppose that $\forall x \in X(\Omega), g(x) = h(x)$. 
* Let $\omega \in \Omega$. 
* Then $X(\omega) \in X(\Omega)$, so $g(X(\omega)) = h(X(\omega))$. 
* Thus, $\forall \omega \in \Omega, g(X(\omega)) = h(X(\omega))$, so $g(X) = h(X)$.

* Now suppose that $g(X) = h(X)$, so $\forall \omega \in \Omega, g(X(\omega)) = h(X(\omega))$. 
* Let $x \in X(\Omega)$.
* Then $\exists \omega \in \Omega$ such that $X(\omega) = x$, so $g(x) = h(x)$.
* Thus, $\forall x \in X(\Omega), g(x) = h(x).$

:::


::: {.definition name="Joint PMF"}
For discrete random variables $X$ and $Y$, the _joint PMF_ of $X$ and $Y$ is

$$f(x,y) = Pr[X=x, Y=y], \forall x,y \in \mathbb{R}.$$
:::


::: {.definition name="Joint CDF"}
For random variables $X$ and $Y$, the _joint CDF_ of $X$ and $Y$ is

$$F(x,y) = Pr[X \leq x, Y \leq y], \forall x,y \in \mathbb{R}.$$
:::


::: {.example name="Flipping a Coin and Rolling a Die"}
Consider the generative process in which the experimenter flips a coin and then rolls either a four-sided or six-sided die depending on the outcome of the coin flip. Let $X = 0$ if the coin comes up heads and $X = 1$ if the coin comes up tails, and let Y be the value of the outcome of the die roll. Then the _joint PMF_ of $X$ and $Y$ is

$$f(x,y) = \begin{cases} \frac{1}{8} &: x = 0, y \in \{1,2,3,4\} \\ \frac{1}{12} &: x = 1, y \in \{1,2,3,4,5,6\} \\ 0 &: otherwise. \end{cases}$$

For discrete random variables, the _joint CDF_ is constructed by summing over the appropriate values of $X$ and $Y$. For example,

$$\begin{align}F(1,3) &=\sum_{x \leq 1}\sum_{y \leq 3} f(x,y) \\ &= f(0,1) + f(0,2) + f(0,3) + f(1,1) + f(1,2) + f(1,3) \\ &= \frac{1}{8} + \frac{1}{8} + \frac{1}{8} + \frac{1}{12} + \frac{1}{12} + \frac{1}{12} \\ &= \frac{5}{8}. \end{align}$$
:::


::: {.theorem name="Marginal PMF"}
For discrete random variables $X$ and $Y$ with joint PMF $f$, the _marginal PMF_ of $Y$ is

$$f_Y(y) = Pr[Y = y] = \sum_{x \in Supp[X]}f(x,y), \forall y \in \mathbb{R}.$$
The proof for this theorem follows directly from the _Law of Total Probability_ where we sum the joint probabilities of $X = x$ and $Y = y$ for every possible outcome for $X$ to get the overall probability that $Y = y$. We can also compute the marginal PMF of $X$ analogously.
:::


::: {.definition name="Conditional PMF"}
For discrete random variables $X$ and $Y$ with joint PMF $f$, the _conditional PMF_ of $Y$ given $X = x$ is

$$f_{Y|X}(y|x) = Pr[Y = y | X = x] = \frac{Pr[X = x, Y = y]}{P[X = x]} = \frac{f(x,y)}{f_X(x)},$$
$\forall y \in \mathbb{R}$ and $\forall x \in Supp[X]$.

Since $\forall x \in Supp[X]$ is equivalent to $f_X(x) > 0$, this domain condition ensures that the denominator is nonzero. The conditional PMF of $X$ given $Y$ is defined analogously.
:::


::: {.example name="Flipping a Coin and Rolling a Die"}
In the coin flip and die roll example, the _marginal PMFs_ are

$$f_X(x) = \begin{cases} \frac{1}{2} &: x = 0 \\ \frac{1}{2} &: x = 1 \\ 0 &: otherwise. \end{cases}$$

and

$$f_Y(y) = \begin{cases} \frac{5}{24} &: y \in \{1,2,3,4\} \\ \frac{1}{12} &: y \in \{5,6\} \\ 0 &: otherwise. \end{cases}$$

and the _conditional PMFs_ are

$$f_{X|Y}(x|y) = \begin{cases} \frac{3}{5} &: x = 0, y \in \{1,2,3,4\} \\ \frac{2}{5} &: x = 1, y \in \{1,2,3,4\} \\ 1 &: x = 1, y \in \{5,6\} \\ 0 &: otherwise. \end{cases}$$
and

$$f_{Y|X}(y|x) = \begin{cases} \frac{1}{4} &: x = 0, y \in \{1,2,3,4\} \\ \frac{1}{6} &: x = 1, y \in \{1,2,3,4,5,6\} \\ 0 &: otherwise. \end{cases}$$

Note that we can operate on _marginal PMFs_ in the same way that we would on _univariate PMFs_, since they are univariate PMFs. Similarly, _conditional PMFS_ are _univariate PMFs_ given any fixed value for the conditioning variable. So, in this example, below is a _univariate PMF_:

$$f_{X|Y}(x|y) = \begin{cases} \frac{3}{5} &: x = 0 \\ \frac{2}{5} &: x = 1 \\ 0 &: otherwise. \end{cases}$$
:::


::: {.theorem name="Multiplicate Law for PMFs"}
We can arrange the definition of _Conditional PMF_ to obtain an analog to the _Multiplicate Law of Probability_ for PMFs. Let $X$ and $Y$ be two discrete random variables with join PMF $f$. Then, $\forall x \in \mathbb{R}$ and $\forall y \in Supp[Y]$,

$$f_{X|Y}(x|y)f_Y(y) = f(x,y).$$
:::


::: {.definition name="Jointly Continuous Random Variables"}
Two random variables $X$ and $Y$ are _jointly continuous_ if there exists a non-negative function $f:\mathbb{R}^2 \to \mathbb{R}$ such that the joint CDF of $X$ and $Y$ is:

$$F(x,y) = Pr[X \leq x, Y \leq y] = \int_{-\infty}^x \int_{-\infty}^y f(u,v) dydu, \forall x,y \in \mathbb{R}.$$
The function $f$ is called the _joint probability density function (joint PDF)_. Just as a single continuous random variable is characterized by a continuous CDF, two jointly continuous random variables are characterized by a continuous joint CDF. Additionally, just as taking the derivative of the CDF of a continuous random variable yields the PDF, taking the mixed second-order partial derivative of the joint CDF of two jointly continuous random variables yields the joint PDF.
:::


::: {.definition name="Joint PDF"}
For jointly continuous random variables $X$ and $Y$ with joint CDF $F$, the _joint PDF_ of $X$ and $Y$ is:

$$f(x,y) = \left.\frac{\delta^2F(u,v)}{\delta u \delta v} \right\vert_{u=x,v=y}, \forall x,y \in \mathbb{R}.$$

More intuitively, as with univariate continuous distributions, event probabilities are computed by _integration_: $\forall a,b,c,d \in \mathbb{R}$ with $a \leq b$ and $c \leq d$,

$$Pr[a \leq X \leq b, c \leq Y \leq d] = \int_a^b \int_c^d f(x,y) dydx.$$

That is, the volume under the PDF over a region equals the probability that $X$ and $Y$ take on values such that $(X,Y)$ is a point in that region. Indeed, the probability of _any_ event can be computed by _integration_.
:::


::: {.theorem name="Marginal PDF"}
We can now define marginal PDFs analogous to marginal PMFs. As in the discrete case, the _marginal PDF_ of $Y$ is just the PDF of $Y$, ignoring the existence of $X$. This theorem shows how a marginal PDF can be derived from a joint PDF. For jointly continuous random variables $X$ and $Y$ with joint PDF $f$, the _marginal PDF_ of $Y$ is:

$$f_Y(y) = \int_{-\infty}^\infty f(x,y) dx, \forall y \in \mathbb{R}.$$
The intuition is analogous to the discrete case: we use integration to "sum" the joint density of $X$ and $Y$ over all values of $x$ to get the overall density for $Y$ at a given $y$. The marginal PDF of $X$ is computed analogously.
:::


::: {.definition name="Conditional PDF"}
By contrast, the _conditional PDF_ of $X$ given $Y$ is the PDF of $X$ _given that_ a certain value fo $Y$ occurs. For jointly continuous random variables $X$ and $Y$ with joint PDF $f$, the _conditional PDF_ of $Y$ given $X=x$ is:

$$f_{Y|X}(y|x) = \frac{f(x,y)}{f_X(x)}, \forall y \in \mathbb{R}, \forall x \in Supp[X].$$
Again, the domain condition $x \in Supp[X]$ ensures that the denominator is nonzero. The conditional PDF of $X$ given $Y=y$ is defined analogously. As in the discrete case, marginal PDFS are univariate PDFs, and conditional PDFs are univariate PDFs given any fixed value for the conditioning variable.
:::


::: {.theorem name="Multiplicate Law for PDFs"}
Let $X$ and $Y$ by two jointly continuous random variables with joint PDF $f$. Then, $\forall x \in \mathbb{R}$ and $\forall y \in Supp[Y]$,

$$f_{X|Y}(x|y)f_Y(y) = f(x,y), \forall y \in \mathbb{R}, \forall x \in Supp[X].$$
:::


::: {.definition name="Independence of Random Variables"}
Let $X$ and $Y$ be either two discrete random variables with joint PMF $f$ or two jointly continuous random variables with joint PDF $f$. Then $X$ and $Y$ are _independent_ if, $\forall x,y \in \mathbb{R}$:

$$f(x,y) = f_X(x)f_Y(y).$$
We write $X \perp \!\!\! \perp Y$ to denote $X$ and $Y$ are independent.
:::


::: {.theorem name="Implications of Independence (Part I)"}
Let $X$ and $Y$ be either two discrete random variables with joint PMF $f$ or two jointly continuous random variables with joint PDF $f$. Then the following statements are equivalent (that is, each one implies all the others):

* $X \perp \!\!\! \perp Y.$
* $\forall x,y \in \mathbb{R}, f(x,y) = f_X(x)f_Y(y).$
* $\forall x \in \mathbb{R}$ and $\forall y \in Supp[Y], f_{X|Y}(x|y) = f_X(x).$
* $\forall D,E \subseteq \mathbb{R},$ the events $\{X \in D\}$ and $\{Y \in E\}$ are independent.
* For all functions $g$ of $X$ and $h$ of $Y$, $g(X) \perp \!\!\! \perp h(Y).$
:::


# Summarizing Distributions

::: {.definition name="Expected Value"}
For a discrete random variable $X$ with probability mass function (PMF) $f$, if $\sum_x \left| x \right| f(x) < \infty$, then the _expected value_ of $X$ is:

$$E[X] = \sum_x xf(x).$$
For a continuous random variable $X$ with probability density function (PDF) $f$, if $\int_{-\infty}^{\infty}\left| x \right| f(x) < \infty$, then the _expected value_ of $X$ is:

$$E[X] = \int_{-\infty}^{\infty}xf(x)dx.$$
The _expected value_ is an operator that takes as an input a random variable and returns a number. So when we compute the expected value of a random variable, we are applying the _expectation operator_.
:::


::: {.example name="A Fair Die Roll"}
Consider a roll of one fair (six-sided) die. Let $X$ be the value of the outcome of the die roll. Then the _expected value_ of $X$ is:

$$
E[X] = \sum_{x=1}^6 xf(x) = 1 \times (\frac{1}{6}) + 2 \times (\frac{1}{6}) + 3 \times (\frac{1}{6}) + 4 \times (\frac{1}{6}) + 5 \times (\frac{1}{6}) + 6 \times (\frac{1}{6}) = \frac{7}{2}.
$$
Note that a random variable does not necessarily take on its expected value with positive probability. In this example, 

$$Pr[X = E[X]] = Pr[X = \frac{7}{2}] = f(\frac{7}{2}) = f(3.5) = 0.$$
:::


::: {.example name="Bernoulli Distribution"}
Let $X$ be a Bernoulli random variable with probability $p$. Then:

$$
E[X] = \sum_{x=1}^1 xf(x) = 0 \times (1 - p) + 1 \times (p) = p.
$$
Note that this implies a convenient feature of Bernoulli random variables: $E[X] = Pr[X = 1].$
:::


::: {.example name="Standard Normal Distribution"}
Let $X \sim N(0,1)$. Then the expected value of $X$ is:

\begin{align}
E[X] &= \int_{-\infty}^{\infty} x \frac{1}{\sqrt {2 \pi}}e^{- \frac{x^2}{2}}dx \\
     &= \frac{1}{\sqrt {2 \pi}} \int_{-\infty}^{\infty} x e^{- \frac{x^2}{2}}dx \\
     &= \left. \frac{1}{\sqrt {2 \pi}} \times (-e^{- \frac{x^2}{2}}) \right\vert_{-\infty}^{\infty} \\
     &= 0.
\end{align}
:::


::: {.theorem name="Expectation of a Function of a Random Variable"}
Since functions of random variables are themselves random variables, they too have expected values. The following theorem establishes how we can compute the expectation of a function of a random variable $g(X)$ without actually deriving the PMF or PDF of $g(X)$.

* If $X$ is a discrete random variable with PMF $f$ and $g$ is a function of $X$, then

$$E[g(X)] = \sum_xg(x)f(x).$$
* If $X$ is a continuous random variable with PDF $f$ and $g$ is a function of $X$, then

$$E[g(X)] = \int_{-\infty}^{\infty}g(x)f(x).$$
:::


::: {.theorem name="Properties of Expected Values"}
For a random variable $X$,

* $\forall c \in \mathbb{R}, E[c] = c.$
* $\forall a \in \mathbb{R}, E[aX] = aE[X].$
:::


::: {.example name="Properties of Expected Values (Proofs)"}
A constant $c$ can be considered as a discrete random variable $X$ with the PMF:

$$
f(x) = 
  \begin{cases}
  1 &: x = c \\
  0 &: otherwise.
  \end{cases}
$$
This is known as a _degenerate distribution_ or a _degenerate random variable_. Thus, $\forall c \in \mathbb{R}$,

$$E[c] = \sum_xxf(x) = c \times f(c) = c \times 1 = c.$$
Now, let $a \in \mathbb{R}$ and let $g(X) = aX$. If $X$ is discrete with PMF $f$, then by the _Theorem of Expectation of a Function of a Random Variable_:

$$E[aX] = E[g(X)] = \sum_xg(x) \times f(x) = \sum_x(ax) \times f(x) = a \sum_x x \times f(x) = aE[X].$$
Likewise, if $X$ is continuous with PDF $f$, then by the same Theorem:

\begin{align}
E[aX] &= E[g(X)] \\
      &= \int_{-\infty}^{\infty} g(x)f(x)dx \\
      &= \int_{-\infty}^{\infty} (ax) \times f(x)dx \\
      &= a \int_{-\infty}^{\infty} x \times f(x)dx \\
      &= aE[X].
\end{align}
:::


::: {.definition name="Expectation of a Bivariate Random Vector"}
For a random vector $(X,Y)$, the _expected value_ of $(X,Y)$ is:

$$E[(X,Y)] = (E[X], E[Y]).$$
Although this definition is rarely used, it illustrates how an operator can be applied to a random vector. More importantly, we can compute the expected value of a function of two random variables, since a function of random variables is itself a random variable.
:::


::: {.theorem name="Expectation of a Function of Two Random Variables"}

* For discrete random variables $X$ and $Y$ with joint PMF $f$, if $h$ is a function of $X$ and $Y$, then:

$$E[h(X,Y)] = \sum_x\sum_yh(x,y)f(x,y).$$
* For continuous random variables $X$ and $Y$ with joint PDF $f$, if $h$ is a function of $X$ and $Y$, then:

$$E[h(X,Y)] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}h(x,y)f(x,y)dydx.$$
:::


::: {.theorem name="Linearity of Expectations"}
Let $X$ and $Y$ be random variables (discrete or continuous). Then, $\forall a,b,c \in \mathbb{R}$,

$$E[aX + bY + c] = aE[X] + bE[Y] + c.$$
:::


::: {.definition name="$j^{th}$ Raw Moment"}
For a random variable $X$ and $j \in \mathbb{N}$, the $j_{th}$ _raw moment_ of $X$ is:

$$\mu_j' = E[X^j].$$
The $j_{th}$ raw moment of a random variable $X$ is the expected value of $X^j$. The expected value is therefore the first raw moment. Raw moments provide summary information about a distribution, describing elements of its shape and location. Sometimes, however, we might seek to have a summary measure that purely reflects the shape and spread of the distribution, and does not depend on its expected value. For $j > 1$, the $j_{th}$ _central moment_ generally provides more useful information about the spread and shape of a distribution than the regular $j_{th}$ moment.
:::


::: {.definition name="$j^{th}$ Central Moment"}
For a random variable $X$ and $j \in \mathbb{N}$, the $j_{th}$ _central moment_ of $X$ is:

$$\mu_j = E[(X - E[X])^j].$$
This is referred to as the central moment because it is centered on $E[X]$. Note that $E[X]$ is the first raw moment, NOT the first central moment. The first central moment of any distribution is E[(X - E[X])] = E[X] - E[X] = 0. When E[X] = 0, then all raw and central moments agree. The sole distriction between raw and central moments lies in whether or not the expected value of X is subtracted before calculations. One of the most common central moments is the second central moment, also known as the _variance_. Whereas the expected value of a distribution characterizes its location and center, _variance_ characterizes its variability or spread. Formally, variance measures the expected value of the squared difference between the observed value $X$ and its mean. Consequently, higher variance implies greater unpredictability.
:::


::: {.definition name="Variance"}
The variance of a random variable $X$ is:

$$V[X] = E[(X - E[X])^2].$$
In other words, the variance is the average squared deviation from the expected value.
:::


::: {.theorem name="Alternative Formula for Variance"}
For a random variable $X$:

$$V[X] = E[X^2] - E[X]^2.$$
Notice that E[X] is a _constant_ and is therefore treated as such. 
:::


::: {.theorem name="Properties of Variance"}
For a random variable $X$:

* $\forall c \in \mathbb{R}, V[X + c] = V[X].$
* $\forall a \in \mathbb{R}, V[aX] = a^2V[X].$
:::


::: {.definition name="Standard Deviation"}
The standard deviation of a random variable $X$ is:

$$\sigma[X] = \sqrt{V[X]}.$$
:::


::: {.definition name="Properties of Standard Deviation"}
For a random variable $X$ is:

* $\forall c \in \mathbb{R}, \sigma[X + c] = \sigma[X].$
* $\forall a \in \mathbb{R}, \sigma[aX] = \left| a \right|\sigma[X].$
:::


::: {.example name="A Fair Die Roll"}
Consider a roll of one fair (six-sided) die. Let $X$ be the value of the outcome of the die roll. Then:

\begin{align}
V[X] &= E[X^2] - E[X]^2 \\
     &= \sum_{x = 1}^6(x^2 \times \frac{1}{6}) - (\sum_{x = 1}^6(x^2 \times \frac{1}{6}))^2 \\
     &= \frac{91}{6} - (\frac{21}{6})^2 \\
     &= \frac{35}{12} \\
     &\approx 2.92.
\end{align}

So,
$$\sigma[X] = \sqrt{V[X]} = \sqrt{2.92} \approx 1.71.$$
:::


::: {.theorem name="Chebyshev's Inequality"}
Let $X$ be a random variable with finite $\sigma[X] > 0$. Then, $\forall \epsilon > 0$,

$$Pr[\left| X - E[X] \right| \geq \epsilon \sigma [X]] \leq \frac{1}{\epsilon^2}.$$
Thus, _Chebyshev's Inequality_ allows us to put an upper bound on the probability that a draw from the distribution will be more than a given number of standard deviations from the mean.
:::


::: {.definition name="Normal Distribution"}
A continuous random variable $X$ follows a normal distribution if it has a PDF:

$$f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{(x - \mu)^2}{2\sigma^2}}, \forall x \in \mathbb{R},$$
for some constants $\mu, \sigma \in \mathbb{R}$ with $\sigma > 0$. We write $X \sim N(\mu, \sigma^2)$ to denote that $X$ follows a normal distribution with parameters $\mu$ and $\sigma$. This implies that knowing the mean and standard deviation of a normal distribution tells us everything about the distribution. 
:::


::: {.theorem name="Mean and Standard Deviation of the Normal Distribution"}
If $X \sim N(\mu, \sigma^2)$, then

* $E[X] = \mu.$
* $\sigma[X] = \sigma.$

The parameters $\mu$ and $\sigma$ of a normal distribution are its mean and standard deviation, respectively. A normal distribution is thus uniquely specified by its mean and standard deviation. Furthermore, this is why $N(0,1)$ is the _standard normal distribution_: it has the "nice" properties of being centerd on zero ($\mu = 0$) and having a standard deviation (and variance) of 1 ($\sigma = \sigma^2 = 1$).
:::


::: {.theorem name="Properties of the Normal Distribution"}
Suppose $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$, then

* $\forall a,b \in \mathbb{R}$ with $a \neq 0$, if $W = aX + b$, then $W \sim N(a\mu_x + b, a^2 \sigma_X^2).$
* If $X \perp \!\!\! \perp Y$ and $Z = X + Y$, then $Z \sim N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2).$

This theorem implies that any linear combination of any number of mutually independent normal random variables must itself be normal.
:::


::: {.definition name="Mean Squared Error (MSE) about c"}
For a random variable $X$ and $c \in \mathbb{R}$, the mean squared error of $X$ about $c$ is $E[(X-c)^2].$
:::


::: {.theorem name="Alternative Formula for MSE"}
For a random variable $X$ and $c \in \mathbb{R}$,

$$E[(X-c)^2] = V[X] + (E[X] - c)^2.$$
Proof:

\begin{align}
E[(X-c)^2] &= E[X^2 -2cX + c^2] \\
           &= E[X^2] - 2cE[X] + c^2 \\
           &= E[X^2] - E[X]^2 + E[X]^2 - 2cE[X] + c^2 \\
           &= (E[X^2] - E[X]^2) + (E[X]^2- 2cE[X] + c^2) \\
           &= V[X] + (E[X] - c)^2. \\
\end{align}

:::


::: {.theorem name="The Expected Value Minimizes MSE"}
For a random variable $X$, the value of $c$ that minimizes the MSE of $X$ about $c$ is $c = E[X]$.

Proof:

\begin{align}
argmin_{c \in \mathbb{R}}E[(X - c)^2] &= argmin_{c \in \mathbb{R}}(V[X] + (E[X] - c)^2) \\
                                      &= argmin_{c \in \mathbb{R}}(E[X] - c)^2 \\
                                      &= E[X].
\end{align}

In other words, if we had to pick one number as a prediction of the value of $X$, the "best" choice (in terms of minimizing MSE) would be E[X].
:::


::: {.example name="A Fair Coin Flip"}
Consider a fair coin flip. Let $X=0$ if the coin comes up heads and $X=1$ if the coin comes up tails. What is the minimum MSE guess for the value of $X$? The PMF of $X$ is:

$$
f(x) =
\begin{cases}
  \frac{1}{2} &: x \in \{0,1\} \\
  0 &: otherwise.
\end{cases}
$$
So the MSE about $c$ is:

\begin{align}
E[(X-c)^2] &= \frac{1}{2}(0-c)^2 + \frac{1}{2}(1-c)^2 \\
           &= \frac{1}{2}(c^2 + 1 + c^2 - 2c) \\
           &= \frac{1}{2}(1 + 2c^2 - 2c) \\
           &= \frac{1}{2} + c^2 - c.
\end{align}

The first-order condition is thus:

$$0 = \frac{d}{dc}E[(X -c)^2] = \frac{d}{dc}(\frac{1}{2} + c^2 - c) = 2c - 1,$$
which is solved by $c = \frac{1}{2}$. This is E[X].
:::


::: {.definition name="Covariance"}
The covariance of two random variables $X$ and $Y$ is:

$$Cov[X,Y] = E[(X - E[X]) (Y - E[Y])].$$
:::


::: {.theorem name="Alternative Formula for Covariance"}
For random variables $X$ and $Y$:

$$Cov[X,Y] = E[XY] - E[X]E[Y].$$
Proof:

\begin{align}
Cov[X,Y] &= E[(X - E[X]) (Y - E[Y])] \\
         &= E[XY - XE[Y] - YE[X] + E[X]E[Y]] \\
         &= E[XY] - E[X]E[Y] - E[X]E[Y] + E[X]E[Y] \\
         &= E[XY] - E[X]E[Y].
\end{align}
:::


::: {.theorem name="Variance Rule"}
Let $X$ and $Y$ be random variables. Then:

$$V[X + Y] = V[X] + 2Cov[X,Y] + V[Y].$$
More generally, $\forall a,b,c \in \mathbb{R}$:

$$V[aX + bY + c] = a^2V[X] + 2abCov[X,Y] + b^2V[Y].$$
Note that, unlike expected values, we do NOT have linearity of variances: $V[aX + bY] \neq aV[X] + bV[Y].$
:::


::: {.theorem name="Properties of Covariance"}
For random variables $X$, $Y$, $Z$, and $W$:

* $\forall c,d \in \mathbb{R}$, $Cov[c,X] = Cov[X,c] = Cov[c,d] = 0.$
* $Cov[X,Y] = Cov[Y,X].$
* $Cov[X,X] = V[X].$
* $\forall a,b,c,d \in \mathbb{R}$, $Cov[aX + c, bY + d] = abCov[X,Y].$
* $Cov[X + W, Y + Z] = Cov[X,Y] + Cov[X,Z] + Cov[W,Y] + Cov[W,Z].$
:::


::: {.definition name="Correlation"}
The correlation of two random variables $X$ and $Y$ with $\sigma[X] > 0$ and $\sigma[Y] > 0$ is:

$$\rho[X,Y] = \frac{Cov[X,Y]}{\sigma[X]\sigma[Y]}.$$
:::


::: {.theorem name="Correlation and Linear Dependence"}
For random variables $X$ and $Y$:

* $\rho[X,Y] \in [-1,1].$
* $\rho[X,Y] = 1 \iff \exists a,b \in \mathbb{R}$ with $b > 0$ such that $Y = a + bX.$
* $\rho[X,Y] = -11 \iff \exists a,b \in \mathbb{R}$ with $b > 0$ such that $Y = a - bX.$
:::


::: {.theorem name="Properties of Correlation"}
For random variables $X$, $Y$, and $Z$:

* $\rho[X,Y] = \rho[Y,X].$
* $\rho[X,X] = 1.$
* $\rho[aX + c, bY + d] = \rho[X,Y], \forall a,b,c,d \in \mathbb{R}$ such that either $a,b > 0$ or $a,b < 0.$
* $\rho[aX + c, bY + d] = \rho[X,Y], \forall a,b,c,d \in \mathbb{R}$ such that either $a < 0 < b$ or $b < 0 < a.$
:::


::: {.theorem name="Implications of Independence (Part II)"}
If $X$ and $Y$ are independent random variables, then:

* $E[XY] = E[X]E[Y].$
* Covariance is zero: $Cov[X,Y] = 0.$
* Correlation is zero: $\rho[X,Y] = 0.$
* Variances are additive: $V[X + Y] = V[X] + V[Y].$
:::


::: {.definition name="Conditional Expectation"}
For discrete random variables $X$ and $Y$ with joint PMF, $f$, the conditional expectation of $Y$ given $X=x$ is:

$$E[Y|X=x] = \sum_y yf_{Y|X}, \forall x \in Supp[X].$$
For jointly continuous random variables $X$ and $Y$ with joint PDF, $f$, the conditional expectation of $Y$ given $X=x$ is:

$$E[Y|X=x] = \int_{-\infty}^{\infty}yf_{Y|X}dy, \forall x \in Supp[X].$$
:::


::: {.theorem name="Conditional Expectation of a Function of Random Variables"}
For discrete random variables $X$ and $Y$ with joint PMF, $f$, if $h$ is a function of $X$ and $Y$, then the conditional expectation of $h(X,Y)$ given $X=x$ is:

$$E[h(X,Y) | X=x] = \sum_yh(x,y)f_{Y|X}(y|x), \forall x \in Supp[X].$$
For jointly continuous random variables $X$ and $Y$ with joint PDF, $f$, if $h$ is a function of $X$ and $Y$, then the conditional expectation of $h(X,Y)$ given $X=x$ is:

$$E[h(X,Y) | X=x] = \int_{-\infty}^{\infty}h(x,y)f_{Y|X}(y|x)dy, \forall x \in Supp[X].$$
:::


::: {.definition name="Conditional Variance"}
For random variables $X$ and $Y$, the conditional variance of $Y$ given $X=x$ is:

$$V[Y | X=x] = E[(Y - E[Y | X=x])^2 | X=x], \forall x \in Supp[X].$$
:::


::: {.theorem name="Alternative Formula for Conditional Variance"}
For random variables $X$ and $Y$, $\forall x \in Supp[X]$:

$$V[Y | X=x] = E[Y^2 | X=x] - E[Y | X=x]^2.$$
:::


::: {.theorem name="Linearity of Conditional Expectations"}
For random variables $X$ and $Y$, if $g$ and $h$ are functions of $X$, then $\forall x \in Supp[X]$:

$$E[g(X)Y+h(X) | X=x] = g(x)E[Y | X=x] + h(x).$$
:::


::: {.proof name="Linearity of Conditional Expectations"}
Let $X$ and $Y$ be either discrete random variables with joint PMF, $f$, or jointly continuous random variables with joint PDF, $f$, and let $g$ and $h$ be functions of $X$. 

If $X$ and $Y$ are discrete, then $\forall x \in Supp[X]$:

\begin{align}
E[g(X)Y+h(X) | X=x] &= \sum_y (g(x)y + h(x)) \times f_{Y|X}(y|x) \\
                    &= g(x) \sum_y yf_{Y|X}(y|x) + h(x) \sum_y f_{Y|X}(y|x) \\
                    &= g(x)E[Y | X=x] + h(x)(1) \\
                    &= g(x)E[Y | X=x] + h(x) \\
                    &= g(X)E[Y|X] + h(X).
\end{align}

Likewise, if $X$ and $Y$ are jointly continuous, then $\forall x \in Supp[X]$:

\begin{align}
E[g(X)Y+h(X) | X=x] &= \int_{-\infty}^{\infty} (g(x)y + h(x)) \times f_{Y|X}(y|x) \\
                    &= g(x) \int_{-\infty}^{\infty} yf_{Y|X}(y|x) + h(x) \int_{-\infty}^{\infty} f_{Y|X}(y|x) \\
                    &= g(x)E[Y | X=x] + h(x)(1) \\
                    &= g(x)E[Y | X=x] + h(x). \\
                    &= g(X)E[Y|X] + h(X).
\end{align}
:::


::: {.definition name="Conditional Expectation Function (CEF)"}
For random variables $X$ and $Y$ with joint PMF/PDF, $f$, the conditional expectation function of $Y$ given $X=x$ is:

$$G_Y(x) = E[Y | X=x], \forall x \in Supp[X].$$
The CEF is a _univariate function_ that maps $x$ to $E[Y | X=x]$. For example, if $X$ is a Bernoulli random variable, then the CEF of $Y$ given $X$ is:

$$G_Y(x) = E[Y | X=x] = \begin{cases} E[Y | X=0] &: x=0 \\ E[Y |X=1] &: x=1. \end{cases}$$
$G_Y(X)$ is a function of the random variable $X$ and is therefore itself a random variable whose value depends on the value of $X$. That is, when $X$ takes on the value $x$, the random variable $G_Y(X)$ takes on the value $G_Y(x) = E[Y | X=x].$ We write $E[Y|X]$ to denote $G_Y(X)$ since $E[Y | X=X]$ would be confusing.
:::


::: {.definition name="Conditional Variance Function (CVF)"}
For random variables $X$ and $Y$ with joint PMF/PDF, $f$, the conditional variance function of $Y$ given $X=x$ is:

$$H_Y(x) = V[Y | X=x], \forall x \in Supp[X].$$
:::


::: {.example name="Flipping a Coin and Rolling a Die"}
Consider again the generative process from previous examples where the conditional PMF of $Y$ given $X=x$ is:
$$f_{Y|X}(y|x) = \begin{cases}\frac{1}{4} &: x=0, y \in \{1,2,3,4\} \\ \frac{1}{6} &: x=1, y \in \{1,2,3,4,5,6\} \\ 0 &: otherwise. \end{cases}$$

Thus, the CEF of $Y$ given $X=x$ is:

\begin{align}
E[Y | X=x] &= \sum_y yf_{Y|X}(y|x) \\
           &= \begin{cases} \sum_{y=1}^4 y \times \frac{1}{4} &: x=0 \\ 
                            \sum_{y=1}^6 y \times \frac{1}{6} &: x=1
                            \end{cases} \\
           &= \begin{cases} \frac{5}{2} &: x=0 \\ 
                            \frac{7}{2} &: x=1.
                            \end{cases}
\end{align}


Likewise the conditional PMF of $X$ given $Y=y$ is:
$$f_{X|Y}(x|y) = \begin{cases}\frac{3}{5} &: x=0, y \in \{1,2,3,4\} \\ \frac{2}{5} &: x=1, y \in \{1,2,3,4\} \\ 1 &: x=1, y \in \{5,6\} \\ 0 &: otherwise. \end{cases}$$
So the CEF of $X$ given $Y=y$ is:

\begin{align}
E[X | Y=y] &= \sum_x xf_{X|Y}(x|y) \\
           &= \begin{cases} 0 \times \frac{3}{5} + 1 \times \frac{2}{5} &: y \in \{1,2,3,4\} \\ 
                            0 \times 1 + 1 \times 1 &: y \in \{5,6\}
                            \end{cases} \\
           &= \begin{cases} \frac{2}{5} &:  y \in \{1,2,3,4\} \\ 
                            1 &: y \in \{5,6\}. 
                            \end{cases}
\end{align}

:::


::: {.theorem name="Law of Iterated Expectations"}
For random variables $X$ and $Y$,

$$E[Y] = E[E[Y|X]].$$
:::


::: {.proof name="Law of Iterated Expectations"}
Let $X$ and $Y$ be either two discrete random variables with joint PMF, $f$, or two jointly continuous random variables with joint PDF, $f$.

If $X$ and $Y$ are discrete, then

\begin{align}
E[Y] &= \sum_y yf_Y(y) \\
     &= \sum_y y \sum_x f(x,y) \\
     &= \sum_x \sum_y yf(x,y) \\
     &= \sum_x \sum_y yf_{Y|X}(y|x)f_X(x) \\
     &= \sum_x (\sum_y yf_{Y|X}(y|x))f_X(x) \\
     &= \sum_x (E[Y | X=x])f_X(x) \\
     &= E[E[Y | X]].
\end{align}

Likewise, if $X$ and $Y$ are jointly continuous, then

\begin{align}
E[Y] &= \int_{-\infty}^{\infty} yf_Y(y)dy \\
     &= \int_{-\infty}^{\infty} y (\int_{-\infty}^{\infty} f_{XY}(x,y)dx) dy \\
     &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} yf_{XY}(x,y)dydx \\
     &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} yf_{Y|X}(y|x) f_X(x)dydx \\
     &= \int_{-\infty}^{\infty} (\int_{-\infty}^{\infty} yf_{Y|X}(y|x)dy) f_X(x)dx \\
     &= \int_{-\infty}^{\infty} (E[Y | X=x]) f_X(x)dx \\
     &= E[E[Y | X]].
\end{align}
:::


::: {.theorem name="Law of Total Variance"}
For random variables $X$ and $Y$,

$$V[Y] = E[V[Y | X]] + V[E[Y | X]].$$
:::


::: {.proof name="Law of Total Variance"}
For random variables $X$ and $Y$,

\begin{align}
V[Y] &= E[Y^2] - E[Y]^2 \\
     &= E[E[Y^2 | X]] - E[E[Y | X]]^2 \\
     &= E[E[Y^2 | X] - E[Y | X]^2 + E[Y | X]^2] - E[E[Y | X]]^2 \\
     &= E[V[Y | X] + E[Y | X]^2] - E[E[Y | X]]^2 \\
     &= E[V[Y | X]] + (E[E[Y | X]^2] - E[E[Y | X]]^2) \\
     &= E[V[Y | X]] + V[E[Y | X]].

\end{align}
:::


::: {.theorem name="Properties of Deviations from the CEF"}
Let $X$ and $Y$ be random variables and let $\epsilon = Y - E[Y | X]$. Then,

* $E[\epsilon | X] = 0$.
* $E[\epsilon] = 0$.
* If $g$ is a function of $X$, then $Cov[g(X), \epsilon] = 0$.
* $V[\epsilon | X] = V[Y | X]$.
* $V[\epsilon] = E[V[Y | X]]$.
:::


::: {.proof name="Properties of Deviations from the CEF"}
Noting that $E[Y|X]$ is a function solely of $X$, $\epsilon = Y - E[Y | X]$, and applying the theorem above,

$$E[\epsilon | X] = E[Y - E[Y|X] | X] = E[Y|X] - E[Y|X] = 0.$$

Applying the Law of Iterated Expectations,
$$E[\epsilon] = E[E[\epsilon | X]] = E[0] = 0.$$
Let $g$ be a function of $X$. Then

\begin{align}
Cov[g(X), \epsilon] &= E[g(X) \epsilon] - E[g(X)]E[\epsilon] \\
                    &= E[g(X)(Y - E[Y|X])] - E[g(X)](0) \\
                    &= E[g(X)Y - g(X)E[Y|X]] \\
                    &= E[g(X)Y] - E[g(X)E[Y|X]] \\
                    &= E[g(X)Y] - E[E[g(X)Y|X]] \\
                    &= E[g(X)Y] - E[g(X)Y] \\
                    &= 0. \\
\end{align}

Recalling the definition of conditional variance, 

\begin{align}
V[\epsilon | X] &= E[(\epsilon - E[\epsilon | X])^2 | X] \\
                &= E[(\epsilon - 0)^2 | X] \\
                &= E[\epsilon ^2 | X] \\
                &= E[(Y - E[Y|X])^2 | X] \\
                &= V[Y | X]. \\
\end{align}

Finally, by the Law of Total Variance,

\begin{align}
V[\epsilon] &= E[V[\epsilon | X]] + V[E[\epsilon | X]] \\
            &= E[V[\epsilon | X]] + V[0] \\
            &= E[V[\epsilon | X]] \\
            &= E[V[Y | X]].
\end{align}
:::


::: {.theorem name="The CEF is the Best Predictor"}
For random variables $X$ and $Y$, the CEF, $E[Y | X]$, is the best (minimum MSE) predictor of $Y$ given $X$.
:::


::: {.proof name="The CEF is the Best Predictor"}
Choose any $g(X)$ to approximate $Y$. Let $U = Y - g(X)$. By definition of minimum MSE, our goal is to choose $g(X)$ to minimize $E[U^2]$. Let $\epsilon = Y - E[Y|X]$ and $W = E[Y|X] - g(X)$, so that $U = \epsilon + W$. ($W$ is a function of $X$, so we can treat it like a constant in expectations conditioned on $X$). Our goal is then to show that, by choosing $g(X) = E[Y|X]$, we will minimize $E[U^2]$. Now,

\begin{align}
E[U^2|X] &= E[(\epsilon + W)^2 | X] \\
         &= E[\epsilon^2 + 2 \epsilon W + W^2 | X] \\
         &= E[\epsilon^2 | X] + 2WE[\epsilon | X] + W^2 \\
         &= E[\epsilon^2 | X] + 0 + W^2 \\
         &= E[\epsilon^2 | X] - E[\epsilon | X]^2 + W^2 \\
         &= V[Y|X] + W^2.
\end{align}

Where the third and fifth lines follow from properties of deviations from the CEF. Thus, applying the Law of Iterated Expectations we get:

$$E[U^2] = E[E[U^2|W]] = E[V[Y|X] + W^2] = E[V[Y|X]] + E[W^2].$$
$E[V[Y|X]]$ does not depend on the choice of $g(X)$. Additionally, $E[W^2] \geq 0$, with equality if $g(X) = E[Y|X]$. Therefore, choosing $g(X) = E[Y|X]$ minimizes MSE.
:::


::: {.theorem name="Best Linear Predictor (BLP)"}
For random variables $X$ and $Y$, if $V[X] > 0$, then the best (minimum MSE) linear predictor of $Y$ given $X$ is $g(X) = \alpha + \beta X$, where

$$\alpha = E[Y] - \frac{Cov[X,Y]}{V[X]}E[X],$$
$$\beta = \frac{Cov[X,Y]}{V[X]}.$$
Note that $\alpha$ is the y-intercept of the BLP, and $\beta$ is its slope. We note two important corollaries:

* The BLP is also the best linear approximation of the CEF; setting $a = \alpha$ and $b = \beta$ minimizes: $E[(E[Y|X] - (a + bX))^2].$
* If the CEF is linear, then the CEF is the BLP.
:::


::: {.theorem name="Properties of Deviations from the BLP"}
Let $X$ and $Y$ be random variables and let $\epsilon = Y - g(X)$, where $g(X)$ is the BLP. Then,

* $E[\epsilon] = 0$.
* $E[X\epsilon] = 0$.
* $Cov[X, \epsilon] = 0$.
:::


::: {.theorem name="Implications of Independence (Part III)"}
If $X$ and $Y$ are independent random variables, then

* $E[Y|X] = E[Y].$
* $V[Y|X] = V[Y].$
* The BLP of $Y$ given $X$ is $E[Y].$
* If $g$ is a function of $X$ and $h$ is a function of $Y$, then 
  * $E[g(Y) | h(X)] = E[g(Y)].$
  * The BLP of $h(Y)$ given $g(X)$ is $E[h(Y)].$
:::


::: {.definition name="Covariance Matrix"}
For a random vector $X$ of length $K$, the covariance matrix $V[X]$ is a matrix whose $(k,k')^{th}$ entry is $Cov[X_{[k]}, X_{[k']}], \forall i,j \in \{1,2,...,K\}$. That is,

$$
V[X] = 
\left(\begin{array}{cc} 

V[X_{[1]}] & V[X_{[1]}, X_{[2]}] & ... & V[X_{[1]}, X_{[K]}] \\
V[X_{[2]}, X_{[1]}] & V[X_{[2]}] & ... & V[X_{[2]}, X_{[K]}] \\
... & ... & ... & ... \\
V[X_{[K]}, X_{[1]}] & V[X_{[K]}, X_{[2]}] & ... & V[X_{[K]}] \\

\end{array}\right)
$$
:::


::: {.theorem name="Multivariate Variance Rule"}
For random variables $X_{[1]}, X_{[2]},..., X_{[K]}$,

$$V[X_{[1]} + X_{[2]} + ... + X_{[K]}] = V[\sum_{k=1}^K X_{[K]}] = \sum_{k=1}^K \sum_{k'=1}^K Cov[X_{[k]}, X_{[k']}].$$
:::


::: {.definition name="Conditional Expectation (Multivariate Case)"}
For discrete random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ with joint PMF, $f$, the conditional expectation of $Y$ given $X=x$ is:

$$E[Y|X=x] = \sum_y f_{Y|X} (y|x), \forall x \in Supp[X].$$

For jointly continuous random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ with joint PDF, $f$, the conditional expectation of $Y$ given $X=x$ is:

$$E[Y|X=x] = \int_{-\infty}^{\infty} f_{Y|X} (y|x)dy, \forall x \in Supp[X].$$
:::


::: {.definition name="CEF (Multivariate Case)"}
For random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ with joint PMF/PDF, $f$, the CEF of $Y$ given $X=x$ is:

$$G_Y(x) = E[Y|X=x], \forall x \in Supp[X].$$
:::


::: {.theorem name="The CEF is the minimum MSE Predictor"}
For random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ the CEF, $E[Y|X]$, is the best (minimum MSE) predictor of $Y$ given $X$.
:::


::: {.theorem name="Coefficients of the BLP are Partial Derivatives"}
For random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ if $g(X)$ is the best linear predictor of $Y$ given $X$, then $\forall k \in \{1,2,...,K\}$,

$$\beta_k = \frac{dg(X)}{dX_{[k]}}.$$
:::


::: {.theorem name="Properties of Deviations from the BLP (Multivariate Case)"}
For random variables $X_{[1]}, X_{[2]},..., X_{[K]}$, and $Y$ if $g(X)$ is the best linear predictor of $Y$ given $X$, then $\forall k \in \{1,2,...,K\}$ and $\epsilon = Y - g(X)$, then

* $E[\epsilon] = 0.$
* $\forall k \in \{1,2,...,K\}, E[X_{[k]}\epsilon] = 0.$
* $\forall k \in \{1,2,...,K\}, Cov[X_{[k]}, \epsilon] = 0.$
:::



# Statistics

::: {.definition name="Independent and Identically Distributed (i.i.d)"}
Let $X_1, X_2, ..., X_n$ be random variables with CDFs $F_1, F_2, ..., F_n$, respectively. Let $F_A$ denote the joint CDF of the random variables with indices in the set $A$. Then $X_1, X_2, ..., X_n$ are independent and identically distributed if they satisfy the following:

* Mutually independent: $\forall A \subseteq \{1,2,...,n\}, \forall \{x_1,x_2,...,x_n\} \in \mathbb{R}^n, F_A((x_i)_{i \in A}) = \prod_{i \in A}F_i(x_i).$
* Identically distributed: $\forall i,j \in \{1,2,...,n\}$ and $\forall x \in \mathbb{R}, F_i(x) = F_j(x).$

In other words, we take a draw from the random variable $X$. Then we take another draw of $X$ in such a manner that our second draw does not depend on the outcome of the first. We repeat this process until we have $n$ draws. So, we have $n$ identical random variables that generate $n$ values.
:::


::: {.definition name="Finite Population Mass Function"}
Given a finite population $U$ with responses $x_1, x_2, ..., x_N$ the finite population mass function,

$$f_{FP}(x) = \frac{1}{N}\sum_{i=1}^NI(x_i = x).$$

That is, $f_{FP}(x)$ is the proportion of units in $U$ that have $x_i=x$. Let $X$ denote the set of unique values of $x_i$, that is $X=\{x \in \mathbb{R}: f_{FP}(x) > 0\}$.

Let $\mu$ denote the population mean of $U$:

$$\mu = \frac{1}{N}\sum_{i=1}^Nx_i = \sum_{x \in X}xf_{FP}(x).$$

Let $\sigma^2$ denote the population variance of $U$:

$$\sigma^2 = \frac{1}{N}\sum_{i=1}^N(x_i - \mu)^2 = \sum_{x \in X}(x - \mu)^2f_{FP}(x).$$
:::


::: {.definition name="Sample Statistic"}
For i.i.d. random variables $X_1, X_2, ..., X_n$, a sample statistic is a function of $X_1, X_2, ..., X_n$:

$$T_{(n)} = h_{(n)}(X_1, X_2, ..., X_n),$$
where $h_{(n)}: \mathbb{R}^n \to \mathbb{R}, \forall n \in \mathbb{N}.$
:::


::: {.definition name="Sample Mean"}
The sample mean of $n$ draws from $X$ can be viewed as an approximation of $E[X]$. It is the average of all the observed values of $X$. For i.i.d. random variables $X_1, X_2, ..., X_n$, the sample mean is

$$\bar{X} = \frac{X_1 + X_2 + ... + X_n}{n} = \frac{1}{n}\sum_{i=1}^nX_i.$$
Keep in mind that the sample mean is a sample statistic and thus is itself a random variable. Depending on the outcomes of the random variables $X_1, X_2, ..., X_n$, $\bar{X}$ will take on different values.
:::


::: {.theorem name="The Expected Value of the Sample Mean is the Population Mean"}
The sample mean tends to approximate the population mean, in the sense that "on average" the value of $\bar{X}$ is $E[X]$. For i.i.d. random variables $X_1, X_2, ..., X_n$,

$$E[\bar{X}] = E[X].$$
:::


::: {.theorem name="Sampling Variance of the Sample Mean"}
As $\bar{X}$ is a random variable, we can describe other features of its distribution. The variance of $\bar{X}$ is known as the sampling variance of the sample mean. Much as the variance characterizes the variability of a random variable $X$, the sampling variance of the sample mean characterizes how much variability we can expect in the sample mean across hypothetical draws of $n$ observations. For i.i.d. random variables $X_1, X_2, ..., X_n$, with finite variance $V[X]$, the sampling variance of $\bar{X}$ is:

$$V[\bar{X}] = \frac{V[X]}{n}.$$
Notice that the sampling variance decreases as $n$ increases.
:::


::: {.theorem name="Chebyshev's Inequality for the Sample Mean"}
Let $X_1, X_2, ..., X_n$ be i.i.d. random variables with finite variance $V[X] > 0$. Then $\forall \epsilon > 0$,

$$Pr[\left| \bar{X} - E[X] \right| \geq \epsilon] \leq \frac{V[X]}{\epsilon^2n}.$$
:::


::: {.definition name="Convergence in Probability"}
Let ($T_{(1)}, T_{(2)}, T_{(3)}...$) be a sequence of random variables and let $c \in \mathbb{R}$. Then $T_{(n)}$ converges in probability to $c$ if $\forall \epsilon > 0$,

$$\lim_{n \to \infty} Pr[\left| T_{(n)} - c \right| \geq \epsilon] = 0,$$
or equivalently,

$$\lim_{n \to \infty} Pr[\left| T_{(n)} - c \right| < \epsilon] = 1.$$

We write $T_{(n)} \to^p c$ to denote that $T_{(n)}$ converges in probability to $c$. The value $c$ to which a sequence of random variables converges in probability is referred to as the probability limit of $T_{(n)}$. Saying that $T_{(n)}$ converges in probability to $c$ means that as $n$ gets large, it becomes increasingly likely that $T_{(n)}$ will be "close" to $c$. 
:::


::: {.theorem name="Continuous Mapping Theorem (CMT)"}
Let ($S_{(1)}, S_{(2)}, S_{(3)}...$) and ($T_{(1)}, T_{(2)}, T_{(3)}...$) be sequences of random variables. Let $g: \mathbb{R}^2 \to \mathbb{R}$ be a continuous function and let $a,b \in \mathbb{R}$. If $S_{(n)} \to^p a$ and $T_{(n)} \to^p b$, then $g(S_{(n)}, T_{(n)}) \to^p g(a,b).$

The CMT says that continuous functions preserve convergence in probability. It also applies to functions of a single random variable and functions of more than two random variables. The CMT will be useful when we seek to generalize to sample statistics other than the sample mean. 
:::


::: {.theorem name="Weak Law of Large Numbers (WLLN)"}
Let $X_1, X_2, ..., X_n$ be i.i.d. random variables with finite variance $V[X] > 0$, and let $\bar{X}_{(n)} = \frac{1}{n}\sum_{i=1}^n X_i.$ Then,

$$\bar{X}_{(n)} \to^p E[X].$$
The WLLN says that as $n$ gets large, the sample mean $\bar{X}$ becomes increasingly likely to approximate $E[X]$ to any arbitrary degree of precision. That is, with a large enough sample, the probability that the sample mean will be far from the population mean will be negligible. The generality of the WLLN shows how the sample mean can be used to estimate the CDF at any point. 
:::


::: {.theorem name="Estimating the CDF"}
Let $X_1, X_2, ..., X_n$ be i.i.d. random variables with common CDF $F$. Let $x \in \mathbb{R}$ and let $Z_i = I(X_i \leq x), \forall i \in \{1,2,...,n\}$, where I() is the indicator function, that is, it takes the value one if its argument is true and zero if it is false. Then,

$$\bar{Z} \to^p E[Z] = F(x).$$
:::


::: {.definition name="Unbiasedness"}
An estimator $\hat{\theta}$ is unbiased for $\theta$ if $E[\hat{\theta}] = 0$. An unbiased estimator gets the right answer "on average." 
:::


::: {.definition name="Bias of an Estimator"}
For an estimator $\hat{\theta}$, the bias of $\hat{\theta}$ is $E[\hat{\theta}] - \theta$. The bias tells us the difference between the "average" value of the estimator and the true value of $\theta$. 
:::


::: {.definition name="Sampling Variance of an Estimator"}
The variance of an estimator is called the sampling variance. For an estimator $\hat{\theta}$, the sampling variance of $\hat{\theta}$ is $V[\hat{\theta}]$. For example, we would say that the sampling variance of the sample mean $\bar{X}$ is $\frac{V[X]}{n}$.
:::


::: {.definition name="Standard Error of an Estimator"}
The standard deviation of an estimator is called the standard error. For an estimator $\hat{\theta}$, the standard error of $\hat{\theta}$ is $\sigma[\hat{\theta}]$. The standard error is a useful metric of variability and will be central to how we make statistical inferences.
:::


::: {.definition name="MSE of an Estimator"}
For an estimator $\hat{\theta}$, the mean squared error (MSE) of $\hat{\theta}$ in estimating $\theta$ is $E[(\hat{\theta} - \theta)^2]$. The MSE of an estimator is the expected squared deviation of the estimate from the true value.
:::


::: {.definition name="Alternative Formula for the MSE of an Estimator"}
For an estimator $\hat{\theta}$, 

$$E[(\hat{\theta} - \theta)^2] = V[\hat{\theta}] + (E[\hat{\theta}] - \theta)^2.$$
The MSE of an estimator is equal to its sampling variance plus the square of its bias. We can thus see why unbiasedness alone is insufficient to ensure that an estimator is sensible: an estimator with zero bias but large sampling variance may often, or even always, be very far off from the true value of $\theta$. All else equal, we prefer estimators with a lower MSE, as that implies that our estimates are better approximations. An estimator that has a lower MSE than another is said to be more efficient.
:::


::: {.definition name="Relative Efficiency"}
Let $\hat{\theta}_A$ and $\hat{\theta}_B$ be estimators of $\theta$. Then $\hat{\theta}_A$ is more efficient than $\hat{\theta}_B$ if it has a lower MSE.
:::


::: {.definition name="Consistency"}
Note that the above definitions all implicitly assume some finite sample size $n$. We can also describe that asymptotic properties of an estimator, that is, the properties of $\hat{\theta}$ as $n \to \infty$. The simplest and perhaps more essential of these properties is consistency. An estimator $\hat{\theta}$ is consistent for $\theta$ if $\hat{\theta} \xrightarrow[\text{}]{\text{p}} \theta$. 

Consistency is the notion that if we had enough data, the probability that our estimate $\hat{\theta}$ would be far from the truth $\theta$ would be close to zero. Although unbiased estimators are not necessarily consistent, any unbiased estimator with $lim_{n \to \infty}V[\hat{\theta}] = 0$ is consistent.
:::


::: {.definition name="Plug-In Sample Variance"}
Let $X_1, X_2, ..., X_n$ be i.i.d. random variables, the plug-in sample variance is $\bar{X^2} - (\bar{X})^2.$ Note that

$$\bar{X^2} = \frac{1}{n}\sum_{i=1}^n X_i^2,$$
that is, the sample mean of $X^2$, and

$$(\bar{X})^2 = (\frac{1}{n}\sum_{i=1}^n X_i)^2,$$
that is, the square of the sample mean of X.
:::


::: {.theorem name="Properties of the Plug-In Sample Variance"}
For i.i.d. random variables $X_1, X_2, ..., X_n$ with finite variance $V[X] > 0$, 

* $E[\bar{X^2} - (\bar{X})^2] = \frac{n-1}{n}V[X].$
* $\bar{X^2} - (\bar{X})^2 \to^p V[X].$

This theorem says that the plug-in sample variance is not unbiased, but nevertheless converges in probability to the true variance under a mild regularity condition. The size of the bias decreases as n increases (since $\frac{n-1}{n} \to 1$ as $n \to \infty$), so in large samples, it becomes negligible. In this setting, it is also easy to correct for this bias.
:::


::: {.definition name="Unbiased Sample Variance"}
For i.i.d. random variables $X_1, X_2, ..., X_n$, the unbiased sample variance is

$$\hat{V}[X] = \frac{n}{n-1}(\bar{X^2} - (\bar{X})^2).$$
:::


::: {.theorem name="Properties of the Unbiased Sample Variance"}
For i.i.d. random variables $X_1, X_2, ..., X_n$ with finite variance $V[X] > 0$,

* $E[\hat{V}[X]] = V[X].$
* $\hat{V}[X] \to^p V[X].$

As we will see, having a consistent estimator of $V[X]$ will allows us to quantify the uncertainty around the estimate of $E[X]$ given by the sample mean.
:::


::: {.definition name="Convergence in Distribution"}
Let $(T_{(1)}, T_{(2)}, T_{(3)},...)$ be a sequence of random variables with CDFs $(F_{(1)}, F_{(2)}, F_{(3)},...)$, and let $T$ be a random variable with CDF $F$. Then $T_{(n)}$ converges in distribution to $T$ if, $\forall t \in \mathbb{R}$ at which $F$ is continuous,

$$\lim_{n \to \infty}F_{(n)}(t) = F(t).$$
We write $T_{(n)} \to^d T$ to denote that $T_{(n)}$ converges in distribution to $T$.
:::


::: {.definition name="Standardized Sample Mean"}
For i.i.d. random variables $X_1, X_2, ..., X_n$ with finite $E[X] = \mu$ and finite variance $V[X] = \sigma^2 > 0$, the standardized sample mean is

$$Z = \frac{(\bar{X} - E[\bar{X}])}{\sigma[\bar{X}]} = \frac{\sqrt{n}(\bar{X} - \mu)}{\sigma}.$$
:::


::: {.theorem name="Estimating the Sampling Variance of the Sample Mean"}
For i.i.d. random variables $X_1, X_2, ..., X_n$ with finite variance $V[X] > 0$, let $\hat{V}[\bar{X}] = \frac{\hat{V}[X]}{n}.$ Then,

* $E[\hat{V}[X]] = V[\bar{X}].$
* $n\hat{V}[\bar{X}] - nV[\bar{X}] = n\hat{V}[\bar{X}] - V[X] \to^p 0.$

The first property states that the sampling variance of the sample mean is unbiased, and the second is equivalent to stating that it is consistent (as the limit distribution of the sample mean has variance V[X]). Thus, it is possible to estimate the average of a random variable (the population mean) and, furthermore, to estimate the uncertainty of that estimate.
:::


::: {.definition name="Standard Error of the Sample Mean"}
For i.i.d. random variables $X_1, X_2, ..., X_n$, the standard error of the sample mean is

$$\sigma[\bar{X}] = \sqrt{V[\bar{X}]}.$$
To estimate the standard error of the sample mean, we use 

$$\hat{\sigma}[\bar{X}] = \sqrt{\hat{V}[\bar{X}]}.$$
:::


::: {.theorem name="Consistency of the Standard Error of the Sample Mean Estimator"}
For i.i.d. random variables $X_1, X_2, ..., X_n$ with finite variance $V[X] > 0$,

$$\sqrt{n}\hat{\sigma}[\bar{X}] - \sqrt{V[X]} = \sqrt{n}\hat{\sigma}[\bar{X}] - \sqrt{n}\sigma[\bar{X}] \to^p 0.$$
:::
