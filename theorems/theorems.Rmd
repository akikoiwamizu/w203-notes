---
title: "Theorems & Definitions"
author: "Akiko Iwamizu"
date: "8/31/2021"
output:
  bookdown::html_document2: default
  bookdown::pdf_document2:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

# Probability

::: {.definition name="Event Space"}
A set $S$ of subsets of $\Omega$ is an _event space_ if it satisfies the following:

* *Nonempty*: $S$ $\ne$ $\emptyset$.
* *Closed under complements*: if $A$ $\in$ $S$, then $A^c$ $\in$ $S$.
* *Closed under countable unions*: if $A_1$, $A_2$, $A_3$,... $\in$ $S$, then $A_1$ $\cup$ $A_2$ $\cup$ $A_3$ $\cup$ ... $\in$ $S$.

&nbsp;

:::

::: {.definition name="Kolmogorov Axioms"}
Let $\Omega$ be a _sample space_, $S$ be an _event space_, and $P$ be a _probability measure_. Then ($\Omega$,$S$,$P$) is a _probability space_ if it satifies the following:

* *Non-negativity*: $\forall$$A$ $\in$ $S$, $P(A) \ge 0$ where $P(A)$ is finite and real.
* *Unitary*: $P(\Omega) = 1$.
* *Countable additivity*: if $A_1$, $A_2$, $A_3$,... $\in$ $S$ are pairwise disjoint, then
$$P(A_1 \cup A_2 \cup A_3 \cup ...) = P(A_1) + P(A_2) + P(A_3) +... = \sum P(A_i).$$

The intuition behind these axioms is as follow: The first axiom states that the probability of any event is a non-negative number; there cannot be a less-than-zero chance of an event occuring. The second axiom states that the probability measure of the entire sample space is one. In other words, it is certain that some outcome will occur. Finally, the third axiom states that, given any number of _mutually exclusive events_, the probability that one of those events will occur is the sum of their individual probabilities.

&nbsp;

:::

::: {.definition name="Pairwise Disjoint"}
Recall that sets $A$ and $B$ are disjoint if $A \cap B = \emptyset$. We say that $A_1$, $A_2$, $A_3$,... are pairwise disjoint if each of them is disjoint from every other, that is, $\forall i \ne j, A_i \cap A_j = \emptyset$.

&nbsp;

:::

::: {.theorem name="Basic Properties of Probability"}
Let ($\Omega$,$S$,$P$) be a probability space. Then

* *Monotonicity*: $\forall A, B \in S$, if $A \subseteq B$, then $P(A) \le P(B)$.
  * Monotonicity implies that, if one event is a subset of another (so that the former always occurs whenever the latter does), then the probability of the former occurring is no greather than that of the latter. 
* *Subtraction rule*: $\forall A, B \in S$, if $A \subseteq B$, then $P(B \setminus A) = P(B) - P(A)$.
  * The subtraction rule implies that the probability that the second event occurs but not the first is equal to the probability of the second event minus the probability of the first event. 
* *Zero probability of the empty set*: $P(\emptyset) = 0$.
  * Zero probability of the empty set means that some event in our event space must occur, and probability bounds mean that each of these events has some probability of occuring between zero and one. 
* *Probability bounds*: $\forall A \in S$, $0 \le P(A) \le 1$.
  * Monotonicity and unitarity (and non-negativity) imply the probability bounds since $A \subseteq \Omega$.
* *Complement rule*: $\forall A \in S$, $P(A^c) = 1 - P(A)$.
  * The complement rule implies that the probability of any of these events not occurring is one minus the probability of the event occurring - so that the probability that a given event either occurs or does not occur is one.

&nbsp;

:::

::: {.definition name="Joint Probability"}
For $A, B \in S$, the _joint probability_ of $A$ and $B$ is $P(A \cap B)$.

In other words, the joint probability of two events $A$ and $B$ is the probability of the intersection of $A$ and $B$ (which is itself an event in $S$), that is, the set of all states of the world in which both $A$ and $B$ occur.

&nbsp;

:::

::: {.theorem name="Addition Rule"}
For $A, B \in S$, 
$$P(A \cup B) = P(A) + P(B) - P(A \cap B).$$

&nbsp;

:::

::: {.definition name="Conditional Probability"}
For $A, B \in S$ with $P(B) > 0$, the _conditional probability_ of $A$ given $B$ is
$$P(A \mid B) = \frac {P(A \cap B)}{P(B)}.$$

&nbsp;

:::

::: {.theorem name="Multiplicative Law of Probability"}
For $A, B \in S$ with $P(B) > 0$,
$$P(A \mid B)P(B) = P(A \cap B).$$
$$P(A \cap B) = P(B|A)P(A).$$

&nbsp;

:::

::: {.theorem name="Bayes' Rule"}
For $A, B \in S$ with $P(A) > 0$ and $P(B) > 0$,
$$P(A \mid B) = \frac {P(B \mid A)P(A)}{P(B)}.$$

&nbsp;

:::

::: {.definition name="Partition Rule 1"}
If $A_1$, $A_2$, $A_3$,... $\in$ $S$ are nonempty and pairwise disjoint, and $\Omega = A_1 \cup A_2 \cup A_3 \cup ...$, then {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$.

$$A = A \cap (B \cup B^c) = (A \cap B) \cup (A \cap B^c) = (A \cap B) \cup (A \setminus B).$$

&nbsp;

:::

::: {.definition name="Partition Rule 2"}
If $A_1$, $A_2$, $A_3$,... $\in$ $S$ are nonempty and pairwise disjoint, and $\Omega = A_1 \cup A_2 \cup A_3 \cup ...$, then {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$.
$$A \cup B = [(A \setminus B) \cup (A \cap B)] \cup [(A \cap B) \cup (B \setminus A)] = (A \setminus B) \cup (A \cap B) \cup (B \setminus A).$$

&nbsp;

:::

::: {.theorem name="Law of Total Probability"}
If {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$ and $B \in S$, then
$$P(B) = \sum P(B \cap A_i).$$

If we also have $P(A_i) > 0$ for $i$ = 1,2,3,..., then this can also be stated as
$$P(B) = \sum P(B \mid A_i)P(A_i).$$

&nbsp;

:::

::: {.theorem name="Alternative Forms of Bayes' Rule"}
If {$A_1$, $A_2$, $A_3$,...} is a _partition_ of $\Omega$ with $P(A_i) > 0$ for $i$ = 1,2,3,..., and $B \in S$, then apply the Law of Total Probability to the denominator to get

$$P(A_j \mid B) = \frac {P(B \mid A_j)P(A_j)}{\sum P(B \cap A_i)}.$$
$$P(A_j \mid B) = \frac {P(B \mid A_j)P(A_j)}{\sum P(B \mid A_i)P(A_i)}.$$
$$P(A \mid B) = \frac {P(B \mid A)P(A)}{P(B \mid A)P(A) + P(B \mid A^c)P(A^c)}.$$

&nbsp;

:::

::: {.definition name="Independence of Events"}
Events $A,B \in S$ are _independent_ if 
$$P(A \cap B) = P(A)P(B).$$

&nbsp;

:::

::: {.theorem name="Conditional Probability and Independence"}
For $A,B \in S$ with $P(B) > 0$, $A$ and $B$ are independent if and only if 
$$P(A \mid B) = P(A).$$

&nbsp;

:::


# Random Variables

::: {.definition name="Random Variable"}
A _random variable_ is a function $X: \Omega \to \mathbb{R}$ such that 

$$\forall r \in \mathbb{R}, \{\omega \in \Omega: X(\omega) \leq r \} \in S.$$ 

Where each $\omega \in \Omega$ denotes a state of the world, which may be represented by anything: numbers, letters, words, etc. to describe all the distinct possible outcomes that could occur. A _random variable_ maps each of these states of the world to a real number. Thus, it is often remarked that, a _random variable_ is neither random nor a variable, as it is merely a _function_. So when the state of the world is $\omega$, the random variable takes on the value $X(\omega)$. For example, the event $\{X = 1\}$ should be understood to mean the set of states $\{\omega \in \Omega: X(\omega) = 1\}$.
:::

::: {.definition name="Function of a Random Variable"}
Let $g: U \to \mathbb{R}$ be a function, where $X(\Omega) \subseteq U \subseteq \mathbb{R}$. Then, if $g \circ X: \Omega \to \mathbb{R}$ is a random variable, we say that $g$ is a _function of X_, and write $g(X)$ to denote the randome variable $g \circ X$. This general definition allows us to formally work with transformations of random variables as random variables in their own right. 
:::

::: {.definition name="Operator on a Random Variable"}
An _operator_ $A$ on a random variable maps the function $X(\cdot)$ to a real number, denoted by $A[X]$.
:::

::: {.example name="Example of a Random Variable"}
We can define events in $S$ in terms of a random variable $X$. For example we could let

* $A = \{\omega \in \Omega: X(\omega) = 1\} = \{X = 1\}.$
* $B = \{\omega \in \Omega: X(\omega) \geq 0\} = \{X \geq 0\}.$
* $C = \{\omega \in \Omega: X(\omega)^2 \le 10, X(\omega) \neq 3\} = \{X(\omega)^2 \le 10, X(\omega) \neq 3\}.$
* $PR[X = 1] = P(A).$
* $PR[X \geq 0] = P(B).$
* $PR[X(\omega)^2 \le 10, X(\omega) \neq 3] = P(C).$
:::

::: {.definition name="Discrete Random Variable"}
A random variable $X$ is _discrete_ if its range, $X(\Omega)$, is a countable set. In other words, a _discrete random variable_ is a random variable that can only take on a finite or countably infinite number of different values.
:::

::: {.definition name="Probability Mass Function (PMF)"}
For a discrete random variable $X$, the _probability mass function_ of $X$ is:

$$f(x) = Pr[X = x], \forall x \in \mathbb{R}.$$
:::

::: {.example name="A Fair Die Roll"}
We can apply the definition of _PMF_ to our familiar die roll example. Consider a roll of one fair (six-sided) die. Let $X$ take on the value of the outcome of the die roll; that is, let $\Omega = \{1,2,3,4,5,6\}$ and $X(\omega) = \omega, \forall \omega \in \Omega$. Then $Pr[X = 1] = Pr[X = 2] = ... = Pr[X = 6] = \frac{1}{6}$. Out of many die rolls, we expect each of the values 1 through 6 to come up one sixth of the time. Thus, the _PMF_ of $X$ is:

$$f(x) = \begin{cases} \frac{1}{6} & : x \in \{1,2,3,4,5,6\} \\ 0 & : otherwise. \end{cases}$$
:::

::: {.example name="A Biased Coin Flip (Bernoulli Distribution)"}
We can also highlight a generalization of our coin flip example: a biased coin flip where the resuling random variable is known as a _Bernoulli_ random variable. Consider a coin flip with a (potentially) biased coin - that is, a coin that comes up tails with some unknown probability. Let $X = 0$ if the coin comes up heads and $X = 1$ if the coin comes up tails; that is, let $\Omega = \{H,T\}$ and let $X(H) = 0$ and $X(T) = 1$. Let $p$ be the probability that the coin comes up tails: $Pr[X = 1] = p$ and let $Pr[X = 0] = 1 - Pr[X = 1] = 1 - p$. Out of many coin flips, we expect that the proportion of times the coin comes up tails will be $p$ and the proportion of the times the coin comes up heads will be $1 - p$. The random variable $X$ thus has a _PMF_ of:

$$f(x) = \begin{cases} 1 - p & : x = 0 \\ p & : x = 1 \\ 0 & : otherwise. \end{cases}$$
:::

::: {.example name="Flipping a Biased Coin Until It Comes Up Tails (Geometric Distribution)"}
This example illustrates how a discrete random variable can have a countably infinite number of possible values. Suppose we flipped a (potentially) biased coin repeatedly until the first time it came up tails. Let $p$ be the probability that the coin comes up tails, and assume $0 < p < 1$. Let $X$ be the number of flips it takes to get tails. For any given positive integer $x$, getting the first tails on exactly the $x^{th}$ flip requires getting heads on each of the first $x - 1$ flips and then tails on the $x^{th}$ flip. The probability of this happening is $(1 - p)^{x - 1}p$, or the product of the probabilities of the desired outcome on each flip (since the flips are independent). So the _PMF_ of $X$ is:

$$f(x) = \begin{cases} (1 - p)^{x - 1}p & : x \in \mathbb{N} \\ 0 & : otherwise. \end{cases}$$
For example, if it is a fair coin (that is, $p = 1 - p = \frac{1}{2}$) then $\forall x \in \mathbb{N}, Pr[X = x] = (\frac{1}{2})^{x}$; the first tails will be obtained on the first flip with probability $\frac{1}{2}$, on the second flip with probability $\frac{1}{4}$, on the third flip with probability $\frac{1}{8}$, and so on. Thus, $X$ can take on _any_ positive integer value, albeit with vanishingly small probability for large values.
:::

::: {.example name="PMF & the Distribution of Discrete Random Variables"}
For a discrete random variable $X$, the _PMF_ tells us everything about its _distribution_, which is loosely defined as the collection of probabilities assigned to events that can be defined just in terms of $X$. To illustrate how the _PMF_ can fully describe a discrete random variable, consider a random variable $X$ such that $f(x) = 0,  \forall x \notin \mathbb{Z}$ (that is, $X$ takes on only integer values). Then:

* $Pr[X \geq 3] = \sum_{x=3}^{\infty}f(x).$
* $Pr[X \geq 3 or X = 1] = f(1) + \sum_{x=3}^{\infty}f(x).$
* $Pr[X < 4] = \sum_{x=1}^{3}f(x).$

For example, for a fair (six-sided) die roll:

* $Pr[X \geq 3] = \sum_{x=3}^{6}f(x) = \frac{4}{6} = \frac{2}{3}.$
* $Pr[X \geq 3 or X = 1] = f(1) + \sum_{x=3}^{6}f(x) = \frac{1}{6} + \frac{4}{6} = \frac{5}{6}.$
* $Pr[X < 4] = \sum_{x=1}^{3}f(x) = \frac{3}{6} = \frac{1}{2}.$
:::


::: {.theorem name="Properties of PMFs"}
For a discrete random variable $X$ with _PMF_ $f$:

* $\forall x \in \mathbb{R}, f(x) \geq 0.$
* $\sum_{x \in X(\Omega)}f(x) = 1.$

The proof of this theorem follows directly from the _Kolmogorov axioms_.  
:::

::: {.theorem name="Event Probabilities for Discrete Random Variables"}
More generally, this theorem gives the formula for using the _PMF_ to compute the probability of _any_ event defined in terms of a discrete random variable $X$. For a discrete random variable $X$ with _PMF_ $f$, if $D \subseteq \mathbb{R}$ and $A = \{X \in D\}$, then:

$$P(A) = Pr[X \in D] = \sum_{x \in X(A)}f(x).$$

The proof of this theorem follows directly from the _Kolmogorov axioms_. Note that any condiiton on $X$ can be expressed as $X \in D$ for some set $D \subseteq \mathbb{R}$, so this theorem allows us to compute the probability of any event defined in terms of a discrete random variable $X$.
:::